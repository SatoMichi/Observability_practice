import json
import string
import time
from typing import List, Dict, Any

import uvicorn
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# BM25 search algorithm
from rank_bm25 import BM25Okapi

# OpenTelemetry imports
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.sdk.resources import Resource
from opentelemetry import propagate
import logging

# NLTKã®åˆæœŸè¨­å®š
import nltk
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

try:
    nltk.data.find('corpora/gutenberg')
except LookupError:
    nltk.download('gutenberg')

from nltk.corpus import gutenberg, stopwords
from nltk.tokenize import word_tokenize, sent_tokenize

# JSONæ§‹é€ åŒ–ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from log_system import setup_logger

# OpenTelemetryã®åˆæœŸåŒ–
class SimpleConsoleSpanExporter:
    """ç°¡æ˜“çš„ãªã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼"""
    def export(self, spans):
        for span in spans:
            print(f"ğŸ” Span: {span.name}")
            print(f"   Service: {span.resource.attributes.get('service.name', 'unknown')}")
            print(f"   Trace ID: {format(span.context.trace_id, '032x')}")
            print(f"   Span ID: {format(span.context.span_id, '016x')}")
            print(f"   Duration: {(span.end_time - span.start_time) / 1_000_000:.2f} ms")
            
            # åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚¹æƒ…å ±ã®è¡¨ç¤º
            distributed_received = span.attributes.get('distributed.trace.received', False)
            if distributed_received:
                traceparent = span.attributes.get('distributed.trace.traceparent', '')
                print(f"   ğŸ”— Distributed Trace: Connected from Frontend")
                print(f"   ğŸ“¡ Traceparent: {traceparent}")
            
            if span.attributes:
                print(f"   Attributes: {dict(span.attributes)}")
            print()
        return 0
    
    def shutdown(self):
        """ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ã®ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³"""
        print("ğŸ” Console Span Exporter shutdown")
        return True

def setup_tracing():
    """OpenTelemetryãƒˆãƒ¬ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
    import os
    
    # ç’°å¢ƒã«å¿œã˜ãŸã‚µãƒ¼ãƒ“ã‚¹åã¨ãƒªã‚½ãƒ¼ã‚¹è¨­å®š
    service_name = os.getenv("OTEL_SERVICE_NAME", "gutenberg-search-api")
    service_version = os.getenv("DD_VERSION", "1.0.0")
    environment = os.getenv("DD_ENV", "development")
    
    # ãƒªã‚½ãƒ¼ã‚¹ã®è¨­å®š
    resource = Resource.create({
        "service.name": service_name,
        "service.version": service_version,
        "deployment.environment": environment
    })
    
    # TracerProviderã®è¨­å®š
    tracer_provider = TracerProvider(resource=resource)
    trace.set_tracer_provider(tracer_provider)
    
    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ã®è¨­å®š
    # ç°¡æ˜“ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›ï¼ˆé–‹ç™ºãƒ»ãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    console_exporter = SimpleConsoleSpanExporter()
    tracer_provider.add_span_processor(BatchSpanProcessor(console_exporter))
    
    # Kubernetesç’°å¢ƒã§ã®åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚¹è¨­å®š
    dd_trace_agent_url = os.getenv("DD_TRACE_AGENT_URL")
    otlp_endpoint = os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT", "http://localhost:4318")
    
    # Datadogç’°å¢ƒã§ã®OTLPè¨­å®š
    if dd_trace_agent_url:
        try:
            # Datadog AgentçµŒç”±ã§ã®ãƒˆãƒ¬ãƒ¼ã‚¹é€ä¿¡
            from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter as HTTPOTLPSpanExporter
            
            # Datadog Agent OTLPã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä½¿ç”¨
            dd_otlp_endpoint = dd_trace_agent_url.replace(":8126", ":4318")
            otlp_exporter = HTTPOTLPSpanExporter(
                endpoint=f"{dd_otlp_endpoint}/v1/traces",
                headers={}
            )
            tracer_provider.add_span_processor(BatchSpanProcessor(otlp_exporter))
            print(f"ğŸ• Datadog OTLP Exporter configured: {dd_otlp_endpoint}")
            
        except Exception as e:
            print(f"âš ï¸  Datadog OTLP Exporter setup failed: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ¨™æº–OTLPã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
            try:
                from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter as HTTPOTLPSpanExporter
                otlp_exporter = HTTPOTLPSpanExporter(
                    endpoint=f"{otlp_endpoint}/v1/traces",
                    headers={}
                )
                tracer_provider.add_span_processor(BatchSpanProcessor(otlp_exporter))
                print(f"ğŸ”— Fallback OTLP Exporter configured: {otlp_endpoint}")
            except Exception as fallback_error:
                print(f"âš ï¸  Fallback OTLP Exporter setup failed: {fallback_error}")
                print("   Continuing with console output only...")
    else:
        # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒç”¨ã®è¨­å®šï¼ˆä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ï¼‰
        print(f"ğŸ’¡ OTLPé€ä¿¡ã‚’ç„¡åŠ¹åŒ–ï¼ˆé–‹ç™ºãƒ¢ãƒ¼ãƒ‰ï¼‰")
        print(f"   ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›ã®ã¿ã§ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°å®Ÿè¡Œä¸­...")
        # try:
        #     from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter as HTTPOTLPSpanExporter
        #     otlp_exporter = HTTPOTLPSpanExporter(
        #         endpoint=f"{otlp_endpoint}/v1/traces",
        #         headers={}
        #     )
        #     tracer_provider.add_span_processor(BatchSpanProcessor(otlp_exporter))
        #     print(f"ğŸ”— Local OTLP Exporter configured: {otlp_endpoint}")
        # except Exception as e:
        #     print(f"âš ï¸  Local OTLP Exporter setup failed: {e}")
        #     print("   Continuing with console output only...")
    
    return trace.get_tracer(__name__)

# ãƒˆãƒ¬ãƒ¼ã‚µãƒ¼ã®åˆæœŸåŒ–
tracer = setup_tracing()

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logger = setup_logger("search_app")

app = FastAPI(title="å…¨æ–‡æ¤œç´¢API")

# FastAPIã®è‡ªå‹•è¨ˆè£…ã‚’æœ‰åŠ¹åŒ–
FastAPIInstrumentor.instrument_app(app)

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:3000"],  # Vite ã¨ CRA ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒ¼ãƒˆ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
books_data = {}
tfidf_vectorizer = None
tfidf_matrix = None
processed_texts = {}
bm25_index = None

def preprocess_text(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†"""
    # å°æ–‡å­—åŒ–
    text = text.lower()
    # å¥èª­ç‚¹ã®é™¤å»
    text = text.translate(str.maketrans('', '', string.punctuation))
    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    tokens = word_tokenize(text)
    # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]
    return ' '.join(tokens)

def get_snippet(text: str, query: str, context_length: int = 25) -> str:
    """æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å«ã‚€ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’å–å¾—"""
    sentences = sent_tokenize(text)
    query_words = query.lower().split()
    
    for sentence in sentences:
        sentence_lower = sentence.lower()
        if any(word in sentence_lower for word in query_words):
            words = sentence.split()
            if len(words) <= context_length * 2:
                return sentence
            
            # ã‚ˆã‚Šè©³ç´°ãªæ¤œç´¢ã‚’è¡Œã„ã€ã‚¯ã‚¨ãƒªãƒ¯ãƒ¼ãƒ‰ã®ä½ç½®ã‚’ç‰¹å®š
            for i, word in enumerate(words):
                if any(qword in word.lower() for qword in query_words):
                    start = max(0, i - context_length)
                    end = min(len(words), i + context_length)
                    snippet = ' '.join(words[start:end])
                    return snippet
    
    # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€åˆã®æ–‡ã®ä¸€éƒ¨ã‚’è¿”ã™
    first_sentence = sentences[0] if sentences else text[:200]
    words = first_sentence.split()
    if len(words) > context_length:
        return ' '.join(words[:context_length]) + "..."
    return first_sentence

@app.on_event("startup")
async def startup_event():
    """ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œ"""
    global books_data, tfidf_vectorizer, tfidf_matrix, processed_texts, bm25_index
    
    with tracer.start_as_current_span("app_startup") as span:
        try:
            start_time = time.time()
            logger.info("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•é–‹å§‹", extra={"event_type": "startup"})
            
            # Gutenbergã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰æ›¸ç±ã‚’å–å¾—
            with tracer.start_as_current_span("load_gutenberg_corpus") as load_span:
                fileids = gutenberg.fileids()
                load_span.set_attribute("corpus.total_files", len(fileids))
                
                for i, fileid in enumerate(fileids):
                    try:
                        with tracer.start_as_current_span("process_book", attributes={"book.id": fileid}):
                            raw_text = gutenberg.raw(fileid)
                            processed_text = preprocess_text(raw_text)
                            
                            # è‘—è€…ã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¨å®šï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ï¼‰
                            if '-' in fileid:
                                parts = fileid.replace('.txt', '').split('-')
                                author = parts[0].replace('_', ' ').title()
                                title = '-'.join(parts[1:]).replace('_', ' ').title() if len(parts) > 1 else fileid
                            else:
                                title = fileid.replace('.txt', '').replace('_', ' ').title()
                                author = "Unknown"
                            
                            books_data[fileid] = {
                                'id': fileid,
                                'title': title,
                                'author': author,
                                'raw_text': raw_text,
                                'word_count': len(raw_text.split())
                            }
                            processed_texts[fileid] = processed_text
                            
                    except Exception as e:
                        logger.error("æ›¸ç±å‡¦ç†ã‚¨ãƒ©ãƒ¼", extra={"event_type": "book_processing_error", "book_id": fileid, "error": str(e)})
            
            # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
            with tracer.start_as_current_span("tfidf_vectorization") as tfidf_span:
                texts_list = list(processed_texts.values())
                tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
                tfidf_matrix = tfidf_vectorizer.fit_transform(texts_list)
                
                tfidf_span.set_attribute("tfidf.max_features", 5000)
                tfidf_span.set_attribute("tfidf.ngram_range", "1,2")
                tfidf_span.set_attribute("tfidf.texts_count", len(texts_list))
                tfidf_span.set_attribute("tfidf.matrix_shape", str(tfidf_matrix.shape))

            # BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
            with tracer.start_as_current_span("bm25_indexing") as bm25_span:
                print(f"ğŸ“Š BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ã‚’é–‹å§‹...")
                bm25_start = time.time()
                
                # BM25ç”¨ã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆæº–å‚™
                tokenized_texts = []
                for text in processed_texts.values():
                    tokenized_texts.append(text.split())
                
                # BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
                bm25_index = BM25Okapi(tokenized_texts)
                
                bm25_time = time.time() - bm25_start
                print(f"ğŸ“Š BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å®Œäº†: {bm25_time:.2f}ç§’")
                print(f"   å¹³å‡æ–‡æ›¸é•·: {bm25_index.avgdl:.1f}ãƒˆãƒ¼ã‚¯ãƒ³")
                print(f"   ç·æ–‡æ›¸æ•°: {len(tokenized_texts)}ä»¶")
                
                bm25_span.set_attribute("bm25.duration_seconds", round(bm25_time, 2))
                bm25_span.set_attribute("bm25.documents_count", len(tokenized_texts))
                bm25_span.set_attribute("bm25.average_doc_length", round(bm25_index.avgdl, 2))
                bm25_span.set_attribute("bm25.total_tokens", sum(len(doc) for doc in tokenized_texts))
            
            total_time = time.time() - start_time
            span.set_attribute("startup.duration_seconds", round(total_time, 2))
            span.set_attribute("startup.books_loaded", len(books_data))
            
            logger.info("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•å®Œäº†", extra={"event_type": "startup_complete", "duration_seconds": round(total_time, 2), "books_count": len(books_data)})
        except Exception as e:
            span.record_exception(e)
            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
            logger.error("èµ·å‹•ã‚¨ãƒ©ãƒ¼", extra={"event_type": "startup_error", "error": str(e)})

@app.get("/")
async def root():
    return {"message": "å…¨æ–‡æ¤œç´¢API"}

@app.get("/books")
async def get_books():
    """å…¨æ›¸ç±ã®æƒ…å ±ã‚’è¿”ã™"""
    start_time = time.time()
    
    books_list = []
    for book_id, book_info in books_data.items():
        books_list.append({
            'id': book_id,
            'title': book_info['title'],
            'author': book_info['author'],
            'word_count': book_info['word_count']
        })
    
    response_time = time.time() - start_time
    logger.info("æ›¸ç±ä¸€è¦§API", extra={"event_type": "api_response", "endpoint": "/books", "response_count": len(books_list), "duration_ms": round(response_time * 1000, 3)})
    
    return {"books": books_list}

def tfidf_search(query: str, max_results: int = 20, similarity_threshold: float = 0.01) -> List[Dict[str, Any]]:
    """TF-IDFãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢ã‚’å®Ÿè¡Œ
    
    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        max_results: æœ€å¤§çµæœä»¶æ•°
        similarity_threshold: é¡ä¼¼åº¦ã®é–¾å€¤
        
    Returns:
        æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
    """
    with tracer.start_as_current_span("tfidf_search") as span:
        span.set_attribute("search.query", query)
        span.set_attribute("search.max_results", max_results)
        span.set_attribute("search.similarity_threshold", similarity_threshold)
        
        # ã‚¯ã‚¨ãƒªã®å‰å‡¦ç†
        with tracer.start_as_current_span("preprocess_query") as preprocess_span:
            processed_query = preprocess_text(query)
            preprocess_span.set_attribute("query.original", query)
            preprocess_span.set_attribute("query.processed", processed_query)
            
            if not processed_query:
                span.set_attribute("search.results_count", 0)
                return []
        
        # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
        with tracer.start_as_current_span("vectorize_query") as vector_span:
            query_vector = tfidf_vectorizer.transform([processed_query])
            vector_span.set_attribute("vector.shape", str(query_vector.shape))
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        with tracer.start_as_current_span("compute_similarity") as similarity_span:
            similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()
            similarity_span.set_attribute("similarity.matrix_size", len(similarities))
        
        # çµæœã®æ•´ç†
        with tracer.start_as_current_span("process_results") as results_span:
            results = []
            book_ids = list(books_data.keys())
            
            for i, similarity in enumerate(similarities):
                if similarity > similarity_threshold:
                    book_id = book_ids[i]
                    book_info = books_data[book_id]
                    
                    # ã‚¹ãƒ‹ãƒšãƒƒãƒˆç”Ÿæˆã‚‚ãƒˆãƒ¬ãƒ¼ã‚¹
                    with tracer.start_as_current_span("generate_snippet", attributes={"book.id": book_id}):
                        snippet = get_snippet(book_info['raw_text'], query)
                    
                    results.append({
                        'id': book_id,
                        'title': book_info['title'],
                        'author': book_info['author'],
                        'score': float(similarity),
                        'snippet': snippet
                    })
            
            # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½çµæœã‚’è¿”ã™
            results.sort(key=lambda x: x['score'], reverse=True)
            final_results = results[:max_results]
            
            results_span.set_attribute("results.total_matches", len(results))
            results_span.set_attribute("results.returned", len(final_results))
            span.set_attribute("search.results_count", len(final_results))
            
            if final_results:
                span.set_attribute("search.top_score", final_results[0]['score'])
                span.set_attribute("search.lowest_score", final_results[-1]['score'])
            
            return final_results

def bm25_search(query: str, max_results: int = 20, score_threshold: float = 0.0) -> List[Dict[str, Any]]:
    """BM25ãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢ã‚’å®Ÿè¡Œï¼ˆTF-IDFã‚ˆã‚Šé«˜ç²¾åº¦ï¼‰
    
    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        max_results: æœ€å¤§çµæœä»¶æ•°
        score_threshold: ã‚¹ã‚³ã‚¢ã®é–¾å€¤
        
    Returns:
        æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
    """
    with tracer.start_as_current_span("bm25_search") as span:
        span.set_attribute("search.query", query)
        span.set_attribute("search.max_results", max_results)
        span.set_attribute("search.score_threshold", score_threshold)
        span.set_attribute("search.algorithm", "BM25")
        
        # ã‚¯ã‚¨ãƒªã®å‰å‡¦ç†
        with tracer.start_as_current_span("preprocess_query") as preprocess_span:
            processed_query = preprocess_text(query)
            preprocess_span.set_attribute("query.original", query)
            preprocess_span.set_attribute("query.processed", processed_query)
            
            if not processed_query:
                span.set_attribute("search.results_count", 0)
                return []
            
            # BM25ç”¨ã«ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
            query_tokens = processed_query.split()
            preprocess_span.set_attribute("query.tokens", query_tokens)
            preprocess_span.set_attribute("query.token_count", len(query_tokens))
        
        # BM25ã‚¹ã‚³ã‚¢è¨ˆç®—
        with tracer.start_as_current_span("compute_bm25_scores") as bm25_span:
            scores = bm25_index.get_scores(query_tokens)
            bm25_span.set_attribute("bm25.scores_count", len(scores))
            bm25_span.set_attribute("bm25.max_score", float(max(scores)) if len(scores) > 0 else 0.0)
            bm25_span.set_attribute("bm25.min_score", float(min(scores)) if len(scores) > 0 else 0.0)
        
        # çµæœã®æ•´ç†
        with tracer.start_as_current_span("process_results") as results_span:
            results = []
            book_ids = list(books_data.keys())
            
            for i, score in enumerate(scores):
                if score > score_threshold:
                    book_id = book_ids[i]
                    book_info = books_data[book_id]
                    
                    # ã‚¹ãƒ‹ãƒšãƒƒãƒˆç”Ÿæˆã‚‚ãƒˆãƒ¬ãƒ¼ã‚¹
                    with tracer.start_as_current_span("generate_snippet", attributes={"book.id": book_id}):
                        snippet = get_snippet(book_info['raw_text'], query)
                    
                    results.append({
                        'id': book_id,
                        'title': book_info['title'],
                        'author': book_info['author'],
                        'score': float(score),
                        'snippet': snippet
                    })
            
            # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½çµæœã‚’è¿”ã™
            results.sort(key=lambda x: x['score'], reverse=True)
            final_results = results[:max_results]
            
            results_span.set_attribute("results.total_matches", len(results))
            results_span.set_attribute("results.returned", len(final_results))
            span.set_attribute("search.results_count", len(final_results))
            
            if final_results:
                span.set_attribute("search.top_score", final_results[0]['score'])
                span.set_attribute("search.lowest_score", final_results[-1]['score'])
            
            logger.info("BM25æ¤œç´¢å®Œäº†", extra={
                "event_type": "bm25_search_complete", 
                "query": query,
                "results_count": len(final_results),
                "total_matches": len(results),
                "top_score": final_results[0]['score'] if final_results else 0.0
            })
            
            return final_results

def slow_tfidf_search(query: str, max_results: int = 20, similarity_threshold: float = 0.01) -> List[Dict[str, Any]]:
    """æ„å›³çš„ã«é…ã„TF-IDFãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢ï¼ˆã‚ªãƒ–ã‚¶ãƒ¼ãƒãƒ“ãƒªãƒ†ã‚£ãƒ¼ç ”ä¿®ç”¨ï¼‰
    
    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        max_results: æœ€å¤§çµæœä»¶æ•°
        similarity_threshold: é¡ä¼¼åº¦ã®é–¾å€¤
        
    Returns:
        æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
    """
    with tracer.start_as_current_span("slow_tfidf_search") as span:
        span.set_attribute("search.query", query)
        span.set_attribute("search.max_results", max_results)
        span.set_attribute("search.similarity_threshold", similarity_threshold)
        span.set_attribute("search.algorithm", "SLOW_TFIDF")
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯1: å‰å‡¦ç†ã§ç„¡é§„ãªå‡¦ç†
        with tracer.start_as_current_span("slow_preprocess_query") as preprocess_span:
            processed_query = preprocess_text(query)
            preprocess_span.set_attribute("query.original", query)
            preprocess_span.set_attribute("query.processed", processed_query)
            
            if not processed_query:
                span.set_attribute("search.results_count", 0)
                return []
            
            # ç„¡é§„ãªæ–‡å­—åˆ—å‡¦ç†ãƒ«ãƒ¼ãƒ—ï¼ˆãƒœãƒˆãƒ«ãƒãƒƒã‚¯ï¼‰
            dummy_operations = 0
            for i in range(50000):  # 5ä¸‡å›ã®ç„¡é§„ãªãƒ«ãƒ¼ãƒ—
                temp_string = processed_query.upper().lower().strip()
                dummy_operations += len(temp_string)
            
            preprocess_span.set_attribute("bottleneck.dummy_operations", dummy_operations)
            time.sleep(0.2)  # 200ms ã®æ„å›³çš„ãªé…å»¶
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯2: ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã§é‡è¤‡å‡¦ç†
        with tracer.start_as_current_span("slow_vectorize_query") as vector_span:
            # é€šå¸¸ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            query_vector = tfidf_vectorizer.transform([processed_query])
            vector_span.set_attribute("vector.shape", str(query_vector.shape))
            
            # ç„¡é§„ãªé‡è¤‡ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆãƒœãƒˆãƒ«ãƒãƒƒã‚¯ï¼‰
            for i in range(10):  # 10å›é‡è¤‡ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–
                duplicate_vector = tfidf_vectorizer.transform([processed_query])
                time.sleep(0.05)  # å„å›50msé…å»¶
            
            vector_span.set_attribute("bottleneck.duplicate_vectorizations", 10)
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯3: é¡ä¼¼åº¦è¨ˆç®—ã§éåŠ¹ç‡ãªå‡¦ç†
        with tracer.start_as_current_span("slow_compute_similarity") as similarity_span:
            # é€šå¸¸ã®é¡ä¼¼åº¦è¨ˆç®—
            similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()
            similarity_span.set_attribute("similarity.matrix_size", len(similarities))
            
            # ç„¡é§„ãªé¡ä¼¼åº¦å†è¨ˆç®—ï¼ˆãƒœãƒˆãƒ«ãƒãƒƒã‚¯ï¼‰
            recalculation_count = 0
            for i in range(5):  # 5å›ç„¡é§„ã«å†è¨ˆç®—
                temp_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()
                recalculation_count += len(temp_similarities)
                time.sleep(0.1)  # å„å›100msé…å»¶
            
            similarity_span.set_attribute("bottleneck.recalculation_operations", recalculation_count)
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯4: çµæœå‡¦ç†ã§éåŠ¹ç‡ãªã‚½ãƒ¼ãƒˆ
        with tracer.start_as_current_span("slow_process_results") as results_span:
            results = []
            book_ids = list(books_data.keys())
            
            for i, similarity in enumerate(similarities):
                if similarity > similarity_threshold:
                    book_id = book_ids[i]
                    book_info = books_data[book_id]
                    
                    # ã‚¹ãƒ‹ãƒšãƒƒãƒˆç”Ÿæˆï¼ˆé€šå¸¸å‡¦ç†ï¼‰
                    with tracer.start_as_current_span("slow_generate_snippet", attributes={"book.id": book_id}):
                        snippet = get_snippet(book_info['raw_text'], query)
                        # å„ã‚¹ãƒ‹ãƒšãƒƒãƒˆç”Ÿæˆå¾Œã«é…å»¶
                        time.sleep(0.03)  # 30msé…å»¶
                    
                    results.append({
                        'id': book_id,
                        'title': book_info['title'],
                        'author': book_info['author'],
                        'score': float(similarity),
                        'snippet': snippet
                    })
            
            # éåŠ¹ç‡ãªãƒãƒ–ãƒ«ã‚½ãƒ¼ãƒˆï¼ˆãƒœãƒˆãƒ«ãƒãƒƒã‚¯ï¼‰
            with tracer.start_as_current_span("inefficient_bubble_sort") as sort_span:
                # ã¾ãšé€šå¸¸ã®ã‚½ãƒ¼ãƒˆï¼ˆã“ã‚Œã¯éš ã™ï¼‰
                results.sort(key=lambda x: x['score'], reverse=True)
                
                # ãã®å¾Œã€æ•™è‚²ç›®çš„ã§è¦‹ãˆã‚‹ãƒãƒ–ãƒ«ã‚½ãƒ¼ãƒˆï¼ˆå®Ÿéš›ã¯ä½•ã‚‚ã—ãªã„ï¼‰
                n = len(results)
                bubble_comparisons = 0
                for i in range(min(n, 50)):  # æœ€å¤§50è¦ç´ ã¾ã§
                    for j in range(min(n-i-1, 50)):
                        bubble_comparisons += 1
                        # å®Ÿéš›ã®äº¤æ›ã¯ã—ãªã„ï¼ˆçµæœã¯å¤‰ã‚ã‚‰ãªã„ã‚ˆã†ã«ï¼‰
                        time.sleep(0.001)  # 1msé…å»¶
                
                sort_span.set_attribute("bottleneck.bubble_sort_comparisons", bubble_comparisons)
            
            final_results = results[:max_results]
            
            results_span.set_attribute("results.total_matches", len(results))
            results_span.set_attribute("results.returned", len(final_results))
            span.set_attribute("search.results_count", len(final_results))
            
            if final_results:
                span.set_attribute("search.top_score", final_results[0]['score'])
                span.set_attribute("search.lowest_score", final_results[-1]['score'])
            
            logger.info("é…ã„TF-IDFæ¤œç´¢å®Œäº†", extra={
                "event_type": "slow_tfidf_search_complete", 
                "query": query,
                "results_count": len(final_results),
                "total_matches": len(results),
                "top_score": final_results[0]['score'] if final_results else 0.0,
                "bottlenecks_included": ["slow_preprocessing", "duplicate_vectorization", "similarity_recalculation", "bubble_sort"]
            })
            
            return final_results

def perform_search(query: str, search_method: str = "tfidf", **kwargs) -> List[Dict[str, Any]]:
    """æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹çµ±åˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    
    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        search_method: æ¤œç´¢æ‰‹æ³• ("tfidf", "bm25", "boolean", "fuzzy" ãªã©)
        **kwargs: å„æ¤œç´¢æ‰‹æ³•å›ºæœ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
    Returns:
        æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
    """
    with tracer.start_as_current_span("perform_search_unified") as span:
        span.set_attribute("search.method", search_method)
        span.set_attribute("search.query", query)
        
        if search_method == "tfidf":
            return tfidf_search(query, **kwargs)
        elif search_method == "bm25":
            return bm25_search(query, **kwargs)
        elif search_method == "slow_tfidf":
            return slow_tfidf_search(query, **kwargs)
        # elif search_method == "boolean":
        #     return boolean_search(query, **kwargs)
        # elif search_method == "fuzzy":
        #     return fuzzy_search(query, **kwargs)
        else:
            available_methods = ["tfidf", "bm25", "slow_tfidf"]
            span.set_status(trace.Status(trace.StatusCode.ERROR, f"Unsupported search method: {search_method}"))
            raise ValueError(f"Unsupported search method: {search_method}. Available methods: {available_methods}")

@app.get("/search")
async def search_books(q: str, method: str = "tfidf", request: Request = None):
    """æ¤œç´¢ã‚¯ã‚¨ãƒªã«åŸºã¥ã„ã¦æ›¸ç±ã‚’æ¤œç´¢
    
    Args:
        q: æ¤œç´¢ã‚¯ã‚¨ãƒª
        method: æ¤œç´¢æ‰‹æ³• ("tfidf" or "bm25")
        request: HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    """
    
    # HTTPãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰ãƒˆãƒ¬ãƒ¼ã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
    context = propagate.extract(dict(request.headers))
    
    # æŠ½å‡ºã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦Spanã‚’é–‹å§‹
    with tracer.start_as_current_span("search_api", context=context) as span:
        span.set_attribute("http.route", "/search")
        span.set_attribute("search.query", q)
        span.set_attribute("search.method", method)
        
        # åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        traceparent = request.headers.get('traceparent')
        tracestate = request.headers.get('tracestate')
        
        if traceparent:
            print(f"ğŸ”— Received Distributed Trace: {traceparent}")
            if tracestate:
                print(f"   Trace State: {tracestate}")
            span.set_attribute("distributed.trace.received", True)
            span.set_attribute("distributed.trace.traceparent", traceparent)
        else:
            print("âš ï¸  No trace context received from frontend")
            span.set_attribute("distributed.trace.received", False)
        
        start_time = time.time()
        
        if not q or not q.strip():
            span.set_attribute("error.type", "validation_error")
            span.set_attribute("error.message", "ç©ºã®æ¤œç´¢ã‚¯ã‚¨ãƒª")
            logger.warning("ç©ºã®æ¤œç´¢ã‚¯ã‚¨ãƒª", extra={"event_type": "search_validation_error", "query": q})
            raise HTTPException(status_code=400, detail="æ¤œç´¢ã‚¯ã‚¨ãƒªãŒç©ºã§ã™")
        
        try:
            # æ¤œç´¢å®Ÿè¡Œ
            with tracer.start_as_current_span("perform_search") as search_span:
                search_span.set_attribute("search.method", method)
                results = perform_search(q, search_method=method)
            
            response_time = time.time() - start_time
            
            # ã‚¹ãƒ‘ãƒ³ã«å±æ€§ã‚’è¿½åŠ 
            span.set_attribute("search.results_count", len(results))
            span.set_attribute("search.response_time_ms", round(response_time * 1000, 3))
            span.set_attribute("http.status_code", 200)
            
            logger.info("æ¤œç´¢API", extra={"event_type": "search_complete", "query": q, "results_count": len(results), "duration_ms": round(response_time * 1000, 3)})
            
            return {
                'query': q,
                'method': method,
                'total_results': len(results),
                'results': results
            }
        except Exception as e:
            error_time = time.time() - start_time
            
            # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ã‚¹ãƒ‘ãƒ³ã«è¨˜éŒ²
            span.record_exception(e)
            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
            span.set_attribute("error.type", type(e).__name__)
            span.set_attribute("error.message", str(e))
            span.set_attribute("search.error_time_ms", round(error_time * 1000, 3))
            span.set_attribute("http.status_code", 500)
            
            logger.error("æ¤œç´¢ã‚¨ãƒ©ãƒ¼", extra={"event_type": "search_error", "query": q, "error": str(e), "duration_ms": round(error_time * 1000, 3)})
            raise HTTPException(status_code=500, detail=f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")

@app.get("/search/compare")
async def compare_search_methods(q: str, request: Request = None):
    """TF-IDFã¨BM25ã®æ¤œç´¢çµæœã‚’æ¯”è¼ƒ
    
    Args:
        q: æ¤œç´¢ã‚¯ã‚¨ãƒª
        request: HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    """
    
    # HTTPãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰ãƒˆãƒ¬ãƒ¼ã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
    context = propagate.extract(dict(request.headers)) if request else {}
    
    with tracer.start_as_current_span("search_compare_api", context=context) as span:
        span.set_attribute("http.route", "/search/compare")
        span.set_attribute("search.query", q)
        
        start_time = time.time()
        
        if not q or not q.strip():
            span.set_attribute("error.type", "validation_error")
            span.set_attribute("error.message", "ç©ºã®æ¤œç´¢ã‚¯ã‚¨ãƒª")
            raise HTTPException(status_code=400, detail="æ¤œç´¢ã‚¯ã‚¨ãƒªãŒç©ºã§ã™")
        
        try:
            # ä¸¦åˆ—ã§ä¸¡æ–¹ã®æ¤œç´¢ã‚’å®Ÿè¡Œ
            with tracer.start_as_current_span("compare_searches") as compare_span:
                
                # TF-IDFæ¤œç´¢
                with tracer.start_as_current_span("tfidf_comparison"):
                    tfidf_start = time.time()
                    tfidf_results = perform_search(q, search_method="tfidf")
                    tfidf_time = time.time() - tfidf_start
                
                # BM25æ¤œç´¢
                with tracer.start_as_current_span("bm25_comparison"):
                    bm25_start = time.time()
                    bm25_results = perform_search(q, search_method="bm25")
                    bm25_time = time.time() - bm25_start
                
                compare_span.set_attribute("tfidf.results_count", len(tfidf_results))
                compare_span.set_attribute("tfidf.duration_ms", round(tfidf_time * 1000, 3))
                compare_span.set_attribute("bm25.results_count", len(bm25_results))
                compare_span.set_attribute("bm25.duration_ms", round(bm25_time * 1000, 3))
            
            response_time = time.time() - start_time
            
            span.set_attribute("search.total_time_ms", round(response_time * 1000, 3))
            span.set_attribute("http.status_code", 200)
            
            logger.info("æ¤œç´¢æ¯”è¼ƒAPI", extra={
                "event_type": "search_compare_complete", 
                "query": q,
                "tfidf_results": len(tfidf_results),
                "bm25_results": len(bm25_results),
                "duration_ms": round(response_time * 1000, 3)
            })
            
            return {
                'query': q,
                'comparison': {
                    'tfidf': {
                        'method': 'tfidf',
                        'total_results': len(tfidf_results),
                        'duration_ms': round(tfidf_time * 1000, 3),
                        'results': tfidf_results[:10]  # ä¸Šä½10ä»¶ã®ã¿è¿”ã™
                    },
                    'bm25': {
                        'method': 'bm25',
                        'total_results': len(bm25_results),
                        'duration_ms': round(bm25_time * 1000, 3),
                        'results': bm25_results[:10]  # ä¸Šä½10ä»¶ã®ã¿è¿”ã™
                    }
                },
                'performance': {
                    'faster_method': 'tfidf' if tfidf_time < bm25_time else 'bm25',
                    'speed_difference_ms': round(abs(tfidf_time - bm25_time) * 1000, 3)
                }
            }
            
        except Exception as e:
            error_time = time.time() - start_time
            
            span.record_exception(e)
            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
            span.set_attribute("error.type", type(e).__name__)
            span.set_attribute("error.message", str(e))
            span.set_attribute("search.error_time_ms", round(error_time * 1000, 3))
            span.set_attribute("http.status_code", 500)
            
            logger.error("æ¤œç´¢æ¯”è¼ƒã‚¨ãƒ©ãƒ¼", extra={"event_type": "search_compare_error", "query": q, "error": str(e), "duration_ms": round(error_time * 1000, 3)})
            raise HTTPException(status_code=500, detail=f"æ¤œç´¢æ¯”è¼ƒã‚¨ãƒ©ãƒ¼: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000) 
