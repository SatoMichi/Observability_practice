import json
import string
import time
from typing import List, Dict, Any
import os
import requests

import uvicorn
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# OpenTelemetry imports
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.sdk.resources import Resource
from opentelemetry import propagate
import logging

# Feature Flags - Unleash
from UnleashClient import UnleashClient

# BM25 Search Algorithm
from rank_bm25 import BM25Okapi

# NLTKã®åˆæœŸè¨­å®š
import nltk
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

try:
    nltk.data.find('corpora/gutenberg')
except LookupError:
    nltk.download('gutenberg')

from nltk.corpus import gutenberg, stopwords
from nltk.tokenize import word_tokenize, sent_tokenize

# JSONæ§‹é€ åŒ–ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from log_system import setup_logger

# OpenTelemetryã®åˆæœŸåŒ–
def setup_tracing():
    """OpenTelemetryãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã®è¨­å®š"""
    
    # ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’è¨­å®š
    resource = Resource(attributes={
        "service.name": "gutenberg-search-api",
        "service.version": "1.0.0",
        "deployment.environment": os.getenv("ENVIRONMENT", "development"),
    })
    
    # ãƒˆãƒ¬ãƒ¼ã‚µãƒ¼ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è¨­å®š
    provider = TracerProvider(resource=resource)
    trace.set_tracer_provider(provider)
    
    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    from opentelemetry.sdk.trace.export import ConsoleSpanExporter
    console_exporter = ConsoleSpanExporter()
    console_processor = BatchSpanProcessor(console_exporter)
    provider.add_span_processor(console_processor)
    
    # OTLP CollectorãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ã¿OTLPã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ã‚’è¿½åŠ 
    try:
        # OTLPã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ã®è¨­å®šã‚’è©¦è¡Œ
        response = requests.get("http://localhost:4318/v1/traces", timeout=1)
        
        # æ¥ç¶šæˆåŠŸã—ãŸå ´åˆã®ã¿OTLPã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ã‚’è¿½åŠ 
        otlp_exporter = OTLPSpanExporter(
            endpoint="http://localhost:4318/v1/traces",
            headers={}
        )
        otlp_processor = BatchSpanProcessor(otlp_exporter)
        provider.add_span_processor(otlp_processor)
        print("ğŸ”— OTLP Exporter configured: http://localhost:4318")
        
    except Exception as e:
        print(f"âš ï¸ OTLP Collector not available, using console output only: {e}")
        print("ğŸ–¥ï¸ Console Exporter configured for tracing")
    
    return trace.get_tracer(__name__)

# ãƒˆãƒ¬ãƒ¼ã‚µãƒ¼ã®åˆæœŸåŒ–
tracer = setup_tracing()

# Unleashã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
def setup_unleash():
    """Unleashã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–"""
    unleash_url = os.getenv("UNLEASH_URL", "http://localhost:4242/api")
    unleash_token = os.getenv("UNLEASH_API_TOKEN", "default:development.unleash-insecure-client-api-token")
    
    try:
        # Unleashã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ï¼ˆèªè¨¼ãªã—ã§ãƒ†ã‚¹ãƒˆï¼‰
        client = UnleashClient(
            url=unleash_url,
            app_name="gutenberg-search-api"
        )
        
        # 5ç§’é–“ã€æ¥ç¶šè©¦è¡Œã‚’å¾…ã¤
        print(f"ğŸ“¡ Unleash client connecting to: {unleash_url}")
        client.initialize_client()
        
        # æ¥ç¶šç¢ºèªã®ãŸã‚ã®å°‘ã—å¾…æ©Ÿ
        import time
        time.sleep(2)
        
        print(f"ğŸš€ Unleash client initialized successfully")
        return client
    except Exception as e:
        print(f"âš ï¸ Unleash client initialization failed: {e}")
        print("   Continuing without feature flags...")
        return None

# Unleashã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
unleash_client = setup_unleash()

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logger = setup_logger("search_app")

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ä½œæˆ
app = FastAPI(
    title="Gutenberg Explorer API",
    description="å¤å…¸æ–‡å­¦æ¤œç´¢ã®ãŸã‚ã®TF-IDFæ¤œç´¢API",
    version="1.0.0"
)

# FastAPIã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ«ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
FastAPIInstrumentor.instrument_app(app)

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:3000"],  # Vite ã¨ CRA ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒ¼ãƒˆ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
books_data = {}
tfidf_vectorizer = None
tfidf_matrix = None
processed_texts = {}
bm25_model = None
tokenized_corpus = []

def preprocess_text(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†"""
    # å°æ–‡å­—åŒ–
    text = text.lower()
    # å¥èª­ç‚¹ã®é™¤å»
    text = text.translate(str.maketrans('', '', string.punctuation))
    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    tokens = word_tokenize(text)
    # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]
    return ' '.join(tokens)

def get_snippet(text: str, query: str, context_length: int = 25) -> str:
    """æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å«ã‚€ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’å–å¾—"""
    sentences = sent_tokenize(text)
    query_words = query.lower().split()
    
    for sentence in sentences:
        sentence_lower = sentence.lower()
        if any(word in sentence_lower for word in query_words):
            words = sentence.split()
            if len(words) <= context_length * 2:
                return sentence
            
            # ã‚ˆã‚Šè©³ç´°ãªæ¤œç´¢ã‚’è¡Œã„ã€ã‚¯ã‚¨ãƒªãƒ¯ãƒ¼ãƒ‰ã®ä½ç½®ã‚’ç‰¹å®š
            for i, word in enumerate(words):
                if any(qword in word.lower() for qword in query_words):
                    start = max(0, i - context_length)
                    end = min(len(words), i + context_length)
                    snippet = ' '.join(words[start:end])
                    return snippet
    
    # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€åˆã®æ–‡ã®ä¸€éƒ¨ã‚’è¿”ã™
    first_sentence = sentences[0] if sentences else text[:200]
    words = first_sentence.split()
    if len(words) > context_length:
        return ' '.join(words[:context_length]) + "..."
    return first_sentence

@app.on_event("startup")
async def startup_event():
    """ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’å®Ÿè¡Œ"""
    global books_data, tfidf_vectorizer, tfidf_matrix, processed_texts, bm25_model, tokenized_corpus
    
    with tracer.start_as_current_span("app_startup") as span:
        try:
            start_time = time.time()
            logger.info("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•é–‹å§‹", extra={"event_type": "startup"})
            
            # Gutenbergã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰æ›¸ç±ã‚’å–å¾—
            with tracer.start_as_current_span("load_gutenberg_corpus") as load_span:
                fileids = gutenberg.fileids()
                load_span.set_attribute("corpus.total_files", len(fileids))
                
                for i, fileid in enumerate(fileids):
                    try:
                        with tracer.start_as_current_span("process_book", attributes={"book.id": fileid}):
                            raw_text = gutenberg.raw(fileid)
                            processed_text = preprocess_text(raw_text)
                            
                            # è‘—è€…ã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¨å®šï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ï¼‰
                            if '-' in fileid:
                                parts = fileid.replace('.txt', '').split('-')
                                author = parts[0].replace('_', ' ').title()
                                title = '-'.join(parts[1:]).replace('_', ' ').title() if len(parts) > 1 else fileid
                            else:
                                title = fileid.replace('.txt', '').replace('_', ' ').title()
                                author = "Unknown"
                            
                            books_data[fileid] = {
                                'id': fileid,
                                'title': title,
                                'author': author,
                                'raw_text': raw_text,
                                'word_count': len(raw_text.split())
                            }
                            processed_texts[fileid] = processed_text
                            
                    except Exception as e:
                        logger.error("æ›¸ç±å‡¦ç†ã‚¨ãƒ©ãƒ¼", extra={"event_type": "book_processing_error", "book_id": fileid, "error": str(e)})
            
            # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
            with tracer.start_as_current_span("tfidf_vectorization") as tfidf_span:
                texts_list = list(processed_texts.values())
                tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
                tfidf_matrix = tfidf_vectorizer.fit_transform(texts_list)
                
                tfidf_span.set_attribute("tfidf.max_features", 5000)
                tfidf_span.set_attribute("tfidf.ngram_range", "1,2")
                tfidf_span.set_attribute("tfidf.texts_count", len(texts_list))
                tfidf_span.set_attribute("tfidf.matrix_shape", str(tfidf_matrix.shape))
            
            # BM25ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
            with tracer.start_as_current_span("bm25_initialization") as bm25_span:
                tokenized_corpus = [text.split() for text in texts_list]
                bm25_model = BM25Okapi(tokenized_corpus)
                
                bm25_span.set_attribute("bm25.corpus_size", len(tokenized_corpus))
                bm25_span.set_attribute("bm25.avg_doc_length", sum(len(doc) for doc in tokenized_corpus) / len(tokenized_corpus))
            
            total_time = time.time() - start_time
            span.set_attribute("startup.duration_seconds", round(total_time, 2))
            span.set_attribute("startup.books_loaded", len(books_data))
            
            logger.info("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•å®Œäº†", extra={"event_type": "startup_complete", "duration_seconds": round(total_time, 2), "books_count": len(books_data)})
        except Exception as e:
            span.record_exception(e)
            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
            logger.error("èµ·å‹•ã‚¨ãƒ©ãƒ¼", extra={"event_type": "startup_error", "error": str(e)})

@app.get("/")
async def root():
    return {"message": "å…¨æ–‡æ¤œç´¢API"}

@app.get("/books")
async def get_books():
    """å…¨æ›¸ç±ã®æƒ…å ±ã‚’è¿”ã™"""
    start_time = time.time()
    
    books_list = []
    for book_id, book_info in books_data.items():
        books_list.append({
            'id': book_id,
            'title': book_info['title'],
            'author': book_info['author'],
            'word_count': book_info['word_count']
        })
    
    response_time = time.time() - start_time
    logger.info("æ›¸ç±ä¸€è¦§API", extra={"event_type": "api_response", "endpoint": "/books", "response_count": len(books_list), "duration_ms": round(response_time * 1000, 3)})
    
    return {"books": books_list}

def tfidf_search(query: str, max_results: int = 20, similarity_threshold: float = 0.01) -> List[Dict[str, Any]]:
    """TF-IDFãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢ã‚’å®Ÿè¡Œ
    
    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        max_results: æœ€å¤§çµæœä»¶æ•°
        similarity_threshold: é¡ä¼¼åº¦ã®é–¾å€¤
        
    Returns:
        æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
    """
    with tracer.start_as_current_span("tfidf_search") as span:
        span.set_attribute("search.query", query)
        span.set_attribute("search.max_results", max_results)
        span.set_attribute("search.similarity_threshold", similarity_threshold)
        
        # ã‚¯ã‚¨ãƒªã®å‰å‡¦ç†
        with tracer.start_as_current_span("preprocess_query") as preprocess_span:
            processed_query = preprocess_text(query)
            preprocess_span.set_attribute("query.original", query)
            preprocess_span.set_attribute("query.processed", processed_query)
            
            if not processed_query:
                span.set_attribute("search.results_count", 0)
                return []
        
        # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
        with tracer.start_as_current_span("vectorize_query") as vector_span:
            query_vector = tfidf_vectorizer.transform([processed_query])
            vector_span.set_attribute("vector.shape", str(query_vector.shape))
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        with tracer.start_as_current_span("compute_similarity") as similarity_span:
            similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()
            similarity_span.set_attribute("similarity.matrix_size", len(similarities))
        
        # çµæœã®æ•´ç†
        with tracer.start_as_current_span("process_results") as results_span:
            results = []
            book_ids = list(books_data.keys())
            
            for i, similarity in enumerate(similarities):
                if similarity > similarity_threshold:
                    book_id = book_ids[i]
                    book_info = books_data[book_id]
                    
                    # ã‚¹ãƒ‹ãƒšãƒƒãƒˆç”Ÿæˆã‚‚ãƒˆãƒ¬ãƒ¼ã‚¹
                    with tracer.start_as_current_span("generate_snippet", attributes={"book.id": book_id}):
                        snippet = get_snippet(book_info['raw_text'], query)
                    
                    results.append({
                        'id': book_id,
                        'title': book_info['title'],
                        'author': book_info['author'],
                        'score': float(similarity),
                        'snippet': snippet,
                        'search_method': 'tfidf'
                    })
            
            # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½çµæœã‚’è¿”ã™
            results.sort(key=lambda x: x['score'], reverse=True)
            final_results = results[:max_results]
            
            results_span.set_attribute("results.total_matches", len(results))
            results_span.set_attribute("results.returned", len(final_results))
            span.set_attribute("search.results_count", len(final_results))
            
            if final_results:
                span.set_attribute("search.top_score", final_results[0]['score'])
                span.set_attribute("search.lowest_score", final_results[-1]['score'])
            
            return final_results

def bm25_search(query: str, max_results: int = 20, score_threshold: float = 0.5) -> List[Dict[str, Any]]:
    """BM25ãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢ã‚’å®Ÿè¡Œ
    
    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        max_results: æœ€å¤§çµæœä»¶æ•°
        score_threshold: BM25ã‚¹ã‚³ã‚¢ã®é–¾å€¤
        
    Returns:
        æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
    """
    with tracer.start_as_current_span("bm25_search") as span:
        span.set_attribute("search.query", query)
        span.set_attribute("search.max_results", max_results)
        span.set_attribute("search.score_threshold", score_threshold)
        span.set_attribute("search.algorithm", "bm25")
        
        # ã‚¯ã‚¨ãƒªã®å‰å‡¦ç†
        with tracer.start_as_current_span("preprocess_query") as preprocess_span:
            processed_query = preprocess_text(query)
            query_tokens = processed_query.split()
            preprocess_span.set_attribute("query.original", query)
            preprocess_span.set_attribute("query.processed", processed_query)
            preprocess_span.set_attribute("query.tokens_count", len(query_tokens))
            
            if not query_tokens:
                span.set_attribute("search.results_count", 0)
                return []
        
        # BM25ã‚¹ã‚³ã‚¢è¨ˆç®—
        with tracer.start_as_current_span("compute_bm25_scores") as bm25_span:
            scores = bm25_model.get_scores(query_tokens)
            bm25_span.set_attribute("bm25.scores_computed", len(scores))
            bm25_span.set_attribute("bm25.max_score", float(max(scores)) if len(scores) > 0 else 0)
            bm25_span.set_attribute("bm25.min_score", float(min(scores)) if len(scores) > 0 else 0)
        
        # çµæœã®æ•´ç†
        with tracer.start_as_current_span("process_results") as results_span:
            results = []
            book_ids = list(books_data.keys())
            
            for i, score in enumerate(scores):
                if score > score_threshold:
                    book_id = book_ids[i]
                    book_info = books_data[book_id]
                    
                    # ã‚¹ãƒ‹ãƒšãƒƒãƒˆç”Ÿæˆã‚‚ãƒˆãƒ¬ãƒ¼ã‚¹
                    with tracer.start_as_current_span("generate_snippet", attributes={"book.id": book_id}):
                        snippet = get_snippet(book_info['raw_text'], query)
                    
                    results.append({
                        'id': book_id,
                        'title': book_info['title'],
                        'author': book_info['author'],
                        'score': float(score),
                        'snippet': snippet,
                        'search_method': 'bm25'
                    })
            
            # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½çµæœã‚’è¿”ã™
            results.sort(key=lambda x: x['score'], reverse=True)
            final_results = results[:max_results]
            
            results_span.set_attribute("results.total_matches", len(results))
            results_span.set_attribute("results.returned", len(final_results))
            span.set_attribute("search.results_count", len(final_results))
            
            if final_results:
                span.set_attribute("search.top_score", final_results[0]['score'])
                span.set_attribute("search.lowest_score", final_results[-1]['score'])
            
            return final_results

def perform_search(query: str, search_method: str = "tfidf", **kwargs) -> List[Dict[str, Any]]:
    """æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹çµ±åˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    
    Args:
        query: æ¤œç´¢ã‚¯ã‚¨ãƒª
        search_method: æ¤œç´¢æ‰‹æ³• ("tfidf", "bm25", "dense" ãªã©)
        **kwargs: å„æ¤œç´¢æ‰‹æ³•å›ºæœ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
    Returns:
        æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
    """
    if search_method == "tfidf":
        return tfidf_search(query, **kwargs)
    elif search_method == "bm25":
        return bm25_search(query, **kwargs)
    # elif search_method == "dense":
    #     return dense_search(query, **kwargs)
    else:
        raise ValueError(f"Unsupported search method: {search_method}")

@app.get("/search")
async def search_books(q: str, request: Request):
    """æ¤œç´¢ã‚¯ã‚¨ãƒªã«åŸºã¥ã„ã¦æ›¸ç±ã‚’æ¤œç´¢"""
    
    # HTTPãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰ãƒˆãƒ¬ãƒ¼ã‚¹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
    context = propagate.extract(dict(request.headers))
    
    # æŠ½å‡ºã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦Spanã‚’é–‹å§‹
    with tracer.start_as_current_span("search_api", context=context) as span:
        span.set_attribute("http.route", "/search")
        span.set_attribute("search.query", q)
        
        # åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        traceparent = request.headers.get('traceparent')
        tracestate = request.headers.get('tracestate')
        
        if traceparent:
            print(f"ğŸ”— Received Distributed Trace: {traceparent}")
            if tracestate:
                print(f"   Trace State: {tracestate}")
            span.set_attribute("distributed.trace.received", True)
            span.set_attribute("distributed.trace.traceparent", traceparent)
        else:
            print("âš ï¸  No trace context received from frontend")
            span.set_attribute("distributed.trace.received", False)
        
        start_time = time.time()
        
        if not q or not q.strip():
            span.set_attribute("error.type", "validation_error")
            span.set_attribute("error.message", "ç©ºã®æ¤œç´¢ã‚¯ã‚¨ãƒª")
            logger.warning("ç©ºã®æ¤œç´¢ã‚¯ã‚¨ãƒª", extra={"event_type": "search_validation_error", "query": q})
            raise HTTPException(status_code=400, detail="æ¤œç´¢ã‚¯ã‚¨ãƒªãŒç©ºã§ã™")
        
        try:
            # ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ•ãƒ©ã‚°ã§æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ±ºå®š
            search_method = "tfidf"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            
            if unleash_client and unleash_client.is_enabled("bm25_search"):
                search_method = "bm25"
                print("ğŸš€ Feature flag enabled: Using BM25 search algorithm")
                logger.info("ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ•ãƒ©ã‚°æœ‰åŠ¹", extra={"event_type": "feature_flag", "flag": "bm25_search", "enabled": True})
            else:
                print("ğŸ“Š Feature flag disabled: Using TF-IDF search algorithm")
                logger.info("ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ•ãƒ©ã‚°ç„¡åŠ¹", extra={"event_type": "feature_flag", "flag": "bm25_search", "enabled": False})
            
            # æ¤œç´¢å®Ÿè¡Œ
            with tracer.start_as_current_span("perform_search") as search_span:
                search_span.set_attribute("search.method", search_method)
                search_span.set_attribute("feature_flag.bm25_enabled", search_method == "bm25")
                results = perform_search(q, search_method=search_method)
            
            response_time = time.time() - start_time
            
            # ã‚¹ãƒ‘ãƒ³ã«å±æ€§ã‚’è¿½åŠ 
            span.set_attribute("search.results_count", len(results))
            span.set_attribute("search.response_time_ms", round(response_time * 1000, 3))
            span.set_attribute("http.status_code", 200)
            
            logger.info("æ¤œç´¢API", extra={"event_type": "search_complete", "query": q, "results_count": len(results), "duration_ms": round(response_time * 1000, 3)})
            
            return {
                'query': q,
                'total_results': len(results),
                'results': results
            }
        except Exception as e:
            error_time = time.time() - start_time
            
            # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ã‚¹ãƒ‘ãƒ³ã«è¨˜éŒ²
            span.record_exception(e)
            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
            span.set_attribute("error.type", type(e).__name__)
            span.set_attribute("error.message", str(e))
            span.set_attribute("search.error_time_ms", round(error_time * 1000, 3))
            span.set_attribute("http.status_code", 500)
            
            logger.error("æ¤œç´¢ã‚¨ãƒ©ãƒ¼", extra={"event_type": "search_error", "query": q, "error": str(e), "duration_ms": round(error_time * 1000, 3)})
            raise HTTPException(status_code=500, detail=f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000) 
